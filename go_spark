2/19/13

To generate 64 bit randoms for Zobrist hashing:  
http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/VERSIONS/C-LANG/mt19937-64.c

randomAction still taking the longest time.  There should be a way to leverage empty_intersections across calls.  (for that matter, empty_intersections should be declared and allocated in .h file anyway, start by making this change anyway. use int8_t)
Then, if nothing is captured by move placement, can reuse empty_intersections and board_ix.  
Else, add the captured (now empty) ixs that have a lower index than the empty pos that was swapped and returned (the new captures that appear after will be gotten to when we need to)

This should be mega faster

Also, put isSuicide inline into applyAction, doesn't get called anywhere else and should be able to reduce the number of neighbor calls

--------------------

Implemented new eye logic per fred's email

uncovered bug, not sure if related, where enemy won't play single spot surrounded by opp color even though it will capture
##fixed

on a related note, say white filled the board except for one spot.  if black played there it would capture.  how can we avoid this?

==============================================================================================

some edits got lost here?

===============================================================

2/9/13

The most time is now spent in random action, presumable iterating through the entire board and randomly shuffling it before testing legality is slow.  Have sped up 2x by being lazy about the Fisher-Yates in-place shuffle.  That is, we generate a random number between 0 and num_empty.  If this falls outside the range of empty values discovered in board, we need to iterate through the board some more add find more empties.  Once we have enough, do the swap and test for legality.  If it fails, we iterate on the empty array with the just failed intersection chopped off the end.  But now, when we generate a rand, it is more likely to fall within our range.  This way, the expected number of positions we need to iterate through on the board, as well as the number of rands and swaps we need to do, is reduced because we stop doing it once an acceptable intersection is found.

=============================================================

2/7/13

Wrestling with floodFill somemore, changed only have one bitMask.  Use another to use results of previous fill when filling neighbor.  For example, if a flood fill from the East neighb runs into stones already explored on the North neighbors FF, we can know for free that the east neighbor string won't be captured.

So BM get and set are being called less, but applyAction is now called ~3 times more often. why?

Maybe related to fact that randomAction doesn't seem to be passing soon enough.  It just sits there spinning its wheels before starting a new game

Also, do short circuit of applyAction that doesn't have to call floodFill, freezeBoard.  i.e all neighbs empty or same  color => auto place if side_effects if true, then done.

Also, if legal already false, don't need to do other test.  Whcih one first though?

--------------

short circuited apply Action
sped up freeze/thawBoard with memcpy
fixed issue with PASS moves not being applied

20k playout / 6s

Further:

can take out adding self stone to isSuicide flood fills, obviated by self short circuiting in applyAction(are you sure?)
can use getNeighbors2 in isSuicide
can avoid hist check if move already illegal

================================================================


2/6/13

Fixed prob with applyAction not rolling back illegal moves

BitMask set and Queue are taking up all the time, what to do?

=============================================================================================

2/5/13

Optimizing wise, 
	-need to fix randomAction (see the 28th)                check
	-fit board rep into bits
	-merge getNeighbors and filter into one unit            check
	-applyAction doesn't need an array, just a bit mask     check
	-give bit masks a length counter                        check

Right, so we are looking at two parallization paths:

1) CPU - root parallelization
	-should be straightforward application of OpenMP pramas.  No shared memory to worry about

2) GPU - leaf parallelization
	-can speed up randomAction by finding obvious legal and illegal moves in parallel, then testing complicated captures in sequence
	- but really applyAction, specifically floodFill, is where the most work is being done
		-TODO: profile to make sure this is really were the bottleneck is
		- speed up repeated filter neighbors action as per above (can be parallelized gains seem miniscule compared to payoff)
		- parallelize floodFill
			- create floodFill kernel, where multiple threads pop intersections off the queue, and add appropriate neighbors onto the queue
				-necessitate locking the queue for safety (but not the BitMasks 'marked' and 'onqueue' because they read or go 0->1)
					-lock struct can be built with atomicAnd()
			- could create one block per opp_color neighbor that needs to be tested for capture (would require separate BitMasks and return booleans, how does work intradevice?)
			-Overhead outweigh the cost?
			
3) GPU - global memory datastructure for more efficient liberty determination
	- not sure how would work, how much more efficient would be


Proposed Lock impl:

lock = false
not_held[num_threads] = [True,...]		

def acquire(thread_id) {
	while( not_held[thread_id] ){
		not_held[thread_id] = atomicAnd( lock, true )
	}
	return
}

=============================================================================================================================================


1/31/13

Good ref here: http://developer.download.nvidia.com/CUDA/training/NVIDIA_GPU_Computing_Webinars_CUDA_Memory_Optimization.pdf

Why wait for GPU to finish playouts before continuing to explore tree?  Better would be asynch, so the CPU keeps doing selection moves.  When the GPU is done
computing the playouts from the previous one, have backprop handler execute?  Not algorithmically precise, but perhaps not a big deal.

cudaMallocHost() ?? pinned host memory (what does this imply, exactly?)

test performance of global vs shared somehow


Started working on integrating threads, can now have multiple threads in block without seg fault
	- still need work on constructors to make warnings go away, this is low priority though

Have the following ideas for speedup:

randomAction : 
	each thread checks an intersection for simple illegality
		-not occupied, not surrounded by kin
		- mark if def legal as well
	randomly shuffle the indices into two groups, def legal and unsure
	while not found legal
		random()
		if rand num falls in def legal, return a member from this group
		else start testing someone from unsure group (maintain walker ix to iterate in order)

applyAction
	have switch to avoid simple eye checking if implicitly already done in randomAction
	have each thread check if has complaining intersection:
		abuts an enemy
		has no liberties
		didn't complain last time
	for each new complainer:
		floodFill to check libs
		if no libs:
			if same color : illegal, return false
			else : capture and keep going (do anything to old_complainer bitmask here?)

	REALLY NEED TO ANALYZE THIS ALGO MORE CAREFULLY, ESP with respect to complainers and captures



=========================================================================================================

1/30/13

Talked to Michael Arnold about GPU. He wants to hava a more complete characterization of them, especially regarding memory access (global vs shared)
So I need to cobble together some sources

Also we need to drill down the algorithms that are relevant here and figure out what can go in parallel.  Perhaps send him the randomAction, applyAction code and formalize the idea of the parallel legal-move checking.

=========================================================================================================

1/29/13

--ptxas-options=-v : reports shared memory per block


========================================================================================================

1/28/13

Will be meeting with Fred and guy to compare my impl to the one discussed here:
http://article.gmane.org/gmane.games.devel.go/20027/match=gpu+go

He got 47,000 playouts per second!? wtf.  I am getting ~600.  Are we measuring different things
or have I done something dumb?  The only real gains I can see are to use Zobrist hashing to compare past states.
But surely this will not be enough....

So made a few improvements:
1) randomAction can be configured to auto-apply the first legal action it comes across
	-whoops this conflicts with the exclusion set.  No matter, need to redo logic for speed-up anyway (see smarterRandomAction for deet, sadly sRA isn't truly random
2) Zobrist Hashing for superko checking
3) simplified rewards calculation to utilize the fact that only single eyes will be open empty when play concludes (by construction)

This got almost a 2x speed up, still a longway off the 47x was looking for :)

TODO: fix/optimize randomAction.  Consider storing open positions in a way that can be randomly sampled (mem loss prob not worth it, got a 8->7 sec when using smarter randomAction, and that is as good sampling a new data structure would be)

Need to do more testing of GPU, not entirely sure it's all working correctly

use deviceQuery for spec's for meeting

use occupancy calculator for more numbers



==================================================================================================

1/27/13

Making high level slides for meeting with that guy at Salk Fred mentioned

TODO: in randomAction(), check for exclusion before doing legality test
TODO: Zobrist hashes rather than storing the past boards explicitly, can improve Ko checking
TODO: root parallelization


===================================================================================================

1/24/13

new direction is to pursue root parallelization
openmp vs boost::threads
	- since will need very little (none) communication between MCTS trees, OpenMP should be easiest
	- can email jack about OpenMP
probably want to fork repo again and remove all CUDA/nvcc related stuff?

Read up on RI learning, fred says I can prob skip chapters 1-3.  
Want to work through DP, MC, then TD approaches

perhaps will need to put some slides together on base code approach by next week


